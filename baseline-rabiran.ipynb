{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer , text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_LABEL = '__label__1'\n",
    "def preprocess_pd(file, nrows):\n",
    "    df = pd.read_csv(file, header=None, delimiter='\\n', nrows=nrows)\n",
    "    # split label and data\n",
    "    df['Y'] = df[0].str[0:10]\n",
    "    df['X'] = df[0].str[11:]\n",
    "    # drop the unsplitted column\n",
    "    df.drop(0,axis=1, inplace=True)\n",
    "    # make labels binary\n",
    "    df.Y = df.Y.apply(lambda label : 0 if label==NEGATIVE_LABEL else 1)\n",
    "    # tokenize\n",
    "    df.X = df.X.apply(text_to_word_sequence)\n",
    "    # get lengths\n",
    "    l = []\n",
    "    for _, row in df.iterrows():\n",
    "        l.append(len(row.X))\n",
    "    df['len'] = l\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_pd('train.ft.txt', 100000)\n",
    "MAX_SEQUENCE_LENGTH = max(df.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all sentences flat (generator)\n",
    "def data_flat_generator(data):\n",
    "    return (w for l in data for w in l)\n",
    "\n",
    "def all_texts_generator():\n",
    "    return (x for x in df.X)\n",
    "\n",
    "def get_word_counts():\n",
    "    return Counter(data_flat_generator(df.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = get_word_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [stuning, even, for, the, non, gamer, this, so...\n",
       "1        [the, best, soundtrack, ever, to, anything, i'...\n",
       "2        [amazing, this, soundtrack, is, my, favorite, ...\n",
       "3        [excellent, soundtrack, i, truly, like, this, ...\n",
       "4        [remember, pull, your, jaw, off, the, floor, a...\n",
       "5        [an, absolute, masterpiece, i, am, quite, sure...\n",
       "6        [buyer, beware, this, is, a, self, published, ...\n",
       "7        [glorious, story, i, loved, whisper, of, the, ...\n",
       "8        [a, five, star, book, i, just, finished, readi...\n",
       "9        [whispers, of, the, wicked, saints, this, was,...\n",
       "10       [the, worst, a, complete, waste, of, time, typ...\n",
       "11       [great, book, this, was, a, great, book, i, ju...\n",
       "12       [great, read, i, thought, this, book, was, bri...\n",
       "13       [oh, please, i, guess, you, have, to, be, a, r...\n",
       "14       [awful, beyond, belief, i, feel, i, have, to, ...\n",
       "15       [don't, try, to, fool, us, with, fake, reviews...\n",
       "16       [a, romantic, zen, baseball, comedy, when, you...\n",
       "17       [fashionable, compression, stockings, after, i...\n",
       "18       [jobst, ultrasheer, thigh, high, excellent, pr...\n",
       "19       [sizes, recomended, in, the, size, chart, are,...\n",
       "20       [mens, ultrasheer, this, model, may, be, ok, f...\n",
       "21       [delicious, cookie, mix, i, thought, it, was, ...\n",
       "22       [another, abysmal, digital, copy, rather, than...\n",
       "23       [a, fascinating, insight, into, the, life, of,...\n",
       "24       [i, liked, this, album, more, then, i, thought...\n",
       "25       [problem, with, charging, smaller, aaas, i, ha...\n",
       "26       [works, but, not, as, advertised, i, bought, o...\n",
       "27       [disappointed, i, read, the, reviews, made, my...\n",
       "28       [oh, dear, i, was, excited, to, find, a, book,...\n",
       "29       [based, on, the, reviews, here, i, bought, one...\n",
       "                               ...                        \n",
       "49970    [6, lcd, digital, caliper, worked, well, the, ...\n",
       "49971    [cheap, and, looks, like, it, this, was, a, ve...\n",
       "49972    [it, worked, great, i, am, glad, i, bought, it...\n",
       "49973    [digital, caliper, for, years, i, have, been, ...\n",
       "49974    [sometimes, it, might, give, the, right, lengt...\n",
       "49975    [idiot, proof, digital, caliper, easy, to, use...\n",
       "49976    [very, poor, quality, i, was, skeptical, of, t...\n",
       "49977    [good, tool, to, lose, on, job, this, is, not,...\n",
       "49978    [digital, caliper, nice, addition, for, my, me...\n",
       "49979    [poor, quality, i, know, it's, not, meant, to,...\n",
       "49980    [cheap, but, waste, of, money, i, knew, it, wa...\n",
       "49981    [so, far, a, failure, i've, yet, to, get, this...\n",
       "49982    [not, great, if, you, need, accurate, repeatab...\n",
       "49983    [not, worth, the, price, this, is, the, worse,...\n",
       "49984    [good, value, for, what, you, get, the, calipe...\n",
       "49985    [nice, quality, i, bought, this, unit, mostly,...\n",
       "49986    [worst, product, i've, ever, purchased, worst,...\n",
       "49987    [cheap, version, this, thing, is, effective, b...\n",
       "49988    [inaccurate, i, bought, this, to, measure, a, ...\n",
       "49989    [great, calipers, there, isn't, really, much, ...\n",
       "49990    [well, well, they, are, cheap, and, you, get, ...\n",
       "49991    [ordered, new, but, received, poorly, refurbis...\n",
       "49992    [excellent, value, how, can, you, go, wrong, f...\n",
       "49993    [great, value, this, caliper, works, great, lo...\n",
       "49994    [eh, good, for, the, price, it, works, ok, som...\n",
       "49995    [from, m, e, student, this, digital, caliper, ...\n",
       "49996    [amazingly, nice, for, the, price, works, grea...\n",
       "49997    [save, your, money, not, accurate, at, all, yo...\n",
       "49998    [great, buy, good, price, fast, shipping, what...\n",
       "49999    [works, well, for, my, application, this, prod...\n",
       "Name: X, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test(file, nrows, word_index, MSL):\n",
    "    df = preprocess_pd(file, nrows)\n",
    "    padded = pad_texts(apply_word2index(df.X, word_index), MSL)\n",
    "    print(len(padded), len(df.Y))\n",
    "    return padded, df.Y\n",
    "    \n",
    "def apply_word2index(texts, word_index):\n",
    "    data =[]\n",
    "    for s in texts:\n",
    "        indexes=[]\n",
    "        for w in s:\n",
    "            indexes.append(word_index[w])\n",
    "        data.append(indexes)\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_index(texts):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)  \n",
    "    word_index = defaultdict(int, tokenizer.word_index)\n",
    "#     sequences = tokenizer.texts_to_sequences(all_texts_generator())\n",
    "#     data = pad_sequences(sequences, MSL)\n",
    "    return word_index\n",
    "\n",
    "def pad_texts(indexed_texts, MSL):\n",
    "        return pad_sequences(indexed_texts, MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = generate_word_index(df.X)\n",
    "input_to_rnn = pad_texts(apply_word2index(df.X, word_index), MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121536"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 100\n",
    "zeros = np.zeros((EMB_SIZE,))\n",
    "def create_embeddings_index(file):\n",
    "    embeddings_index = {}\n",
    "    f = open(file, encoding='UTF-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = create_embeddings_index('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_matrix(word_index, emb_index, EMBEDDING_DIM=EMB_SIZE):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = emb_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embeddings_matrix(word_index, emb_index=emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain, xtest, ytrain, ytest = train_test_split(input_to_rnn, df.Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 400000\n"
     ]
    }
   ],
   "source": [
    "Xtest, Ytest = prepare_test('test.ft.txt', 400000, word_index.copy(), MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNCustom(word_index={}, embedding_weights=[],input_dim = 100,Hidden_Layer_Size= 64,\n",
    "              timesteps=8, RLAYER=None, ACTIVATION=\"sigmoid\",\n",
    "              LOSS=\"binary_crossentropy\",OPTIMIZER='nadam',METRICS=[\"accuracy\"]):\n",
    "    from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.layers.recurrent import LSTM\n",
    "    from keras.models import Sequential\n",
    "    \n",
    "    RLAYER = LSTM if RLAYER is None else RLAYER\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,  # vocab size \n",
    "                            input_dim,\n",
    "                            weights = [embedding_weights],\n",
    "                            input_length = timesteps,\n",
    "                            trainable = False))\n",
    "    model.add(RLAYER(Hidden_Layer_Size))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(ACTIVATION))\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER,metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNCustom(word_index=word_index, embedding_weights=embedding_matrix, input_dim=EMB_SIZE, \n",
    "                  timesteps=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121536"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "100000/100000 [==============================] - 262s 3ms/step - loss: 0.3535 - acc: 0.8411\n",
      "Epoch 2/4\n",
      "100000/100000 [==============================] - 265s 3ms/step - loss: 0.2514 - acc: 0.8964\n",
      "Epoch 3/4\n",
      "100000/100000 [==============================] - 283s 3ms/step - loss: 0.2217 - acc: 0.9098\n",
      "Epoch 4/4\n",
      "100000/100000 [==============================] - 281s 3ms/step - loss: 0.1992 - acc: 0.9204\n",
      "400000/400000 [==============================] - 544s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2214288782683015, 0.9116525]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 4\n",
    "model.fit(input_to_rnn, df.Y, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "model.evaluate(x=Xtest, y=Ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
